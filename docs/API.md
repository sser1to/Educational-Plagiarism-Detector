# API Documentation

## PlagiarismDetector Class

–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞ –≤ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö.

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

```python
from src.main import PlagiarismDetector

detector = PlagiarismDetector(language='english')
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `language` (str): –Ø–∑—ã–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: `'english'`, `'russian'`. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `'english'`

### –ú–µ—Ç–æ–¥—ã

#### load_documents(source, recursive=False)

–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞, —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `source` (str | List[str]): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
- `recursive` (bool): –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `False`

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Dict[str, str]` - —Å–ª–æ–≤–∞—Ä—å {–∏–º—è_—Ñ–∞–π–ª–∞: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ}

**–ü—Ä–∏–º–µ—Ä:**
```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
detector.load_documents('uploads/')

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
detector.load_documents(['file1.txt', 'file2.txt'])

# –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
detector.load_documents('data/', recursive=True)
```

#### process_documents(remove_stopwords=True, lemmatize=True)

–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `remove_stopwords` (bool): –£–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `True`
- `lemmatize` (bool): –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `True`

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Dict[str, List[str]]` - —Å–ª–æ–≤–∞—Ä—å {–∏–º—è_—Ñ–∞–π–ª–∞: —Å–ø–∏—Å–æ–∫_—Ç–æ–∫–µ–Ω–æ–≤}

**–ü—Ä–∏–º–µ—Ä:**
```python
tokens = detector.process_documents(
    remove_stopwords=True,
    lemmatize=True
)
```

#### compare_documents()

–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É —Å–æ–±–æ–π.

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Dict` - —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- `document_names`: —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- `cosine_similarity`: –º–∞—Ç—Ä–∏—Ü–∞ cosine similarity
- `sequence_matcher`: –º–∞—Ç—Ä–∏—Ü–∞ sequence matcher
- `bigram_similarity`: –º–∞—Ç—Ä–∏—Ü–∞ bigram similarity
- `average_similarity`: –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ä–µ–¥–Ω–µ–π —Å—Ö–æ–∂–µ—Å—Ç–∏

**–ü—Ä–∏–º–µ—Ä:**
```python
results = detector.compare_documents()
```

#### compare_two_documents(doc1_name, doc2_name)

–î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `doc1_name` (str): –ò–º—è –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- `doc2_name` (str): –ò–º—è –≤—Ç–æ—Ä–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Dict[str, float]` - —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏:
- `cosine_tfidf`: Cosine similarity —Å TF-IDF
- `sequence_matcher`: SequenceMatcher ratio
- `lcs_length`: –î–ª–∏–Ω–∞ longest common subsequence
- `lcs_percent`: –ü—Ä–æ—Ü–µ–Ω—Ç LCS
- `bigram_similarity`: Bigram Jaccard similarity
- `trigram_similarity`: Trigram Jaccard similarity
- `average_similarity`: –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å

**–ü—Ä–∏–º–µ—Ä:**
```python
details = detector.compare_two_documents('student1.txt', 'student2.txt')
print(f"Cosine similarity: {details['cosine_tfidf']:.2%}")
```

#### visualize_results(save_dir=None, threshold=0.7)

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `save_dir` (str | None): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤. –ï—Å–ª–∏ `None`, –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ
- `threshold` (float): –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `0.7`

**–ü—Ä–∏–º–µ—Ä:**
```python
detector.visualize_results(save_dir='results/', threshold=0.7)
```

#### generate_report(threshold=0.7)

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –ø–ª–∞–≥–∏–∞—Ç–µ.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `threshold` (float): –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `0.7`

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Dict` - —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞—Ö:
- `total_documents`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- `suspicious_pairs`: —Å–ø–∏—Å–æ–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
- `threshold`: –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–æ—Ä–æ–≥

**–ü—Ä–∏–º–µ—Ä:**
```python
report = detector.generate_report(threshold=0.6)
print(f"–ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä: {len(report['suspicious_pairs'])}")
```

#### print_report(threshold=0.7)

–í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ –ø–ª–∞–≥–∏–∞—Ç–µ –≤ –∫–æ–Ω—Å–æ–ª—å.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `threshold` (float): –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `0.7`

**–ü—Ä–∏–º–µ—Ä:**
```python
detector.print_report(threshold=0.7)
```

---

## –§—É–Ω–∫—Ü–∏–∏

### analyze_plagiarism(source, language='english', threshold=0.7, visualize=True, save_dir=None)

–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `source` (str | List[str]): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
- `language` (str): –Ø–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `'english'`
- `threshold` (float): –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `0.7`
- `visualize` (bool): –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: `True`
- `save_dir` (str | None): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `PlagiarismDetector` - –æ–±—ä–µ–∫—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

**–ü—Ä–∏–º–µ—Ä:**
```python
from src.main import analyze_plagiarism

detector = analyze_plagiarism(
    'uploads/',
    language='english',
    threshold=0.7,
    visualize=True,
    save_dir='results/'
)
```

### analyze_data(data_path)

–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å README).

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `data_path` (str): –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `dict` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞

**–ü—Ä–∏–º–µ—Ä:**
```python
from src.main import analyze_data

result = analyze_data("uploads/")
print(result)
```

---

## –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã

### TextProcessor

–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤.

**–ú–µ—Ç–æ–¥—ã:**
- `clean_text(text)`: –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç URL, email, –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
- `tokenize(text)`: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- `remove_stopwords(tokens)`: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
- `lemmatize(tokens)`: –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤
- `process(text, remove_stopwords=True, lemmatize=True)`: –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- `get_ngrams(tokens, n=2)`: –ü–æ–ª—É—á–µ–Ω–∏–µ n-–≥—Ä–∞–º–º
- `get_word_frequency(tokens)`: –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤

### DocumentLoader

–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.

**–ú–µ—Ç–æ–¥—ã:**
- `load(file_path)`: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞)
- `load_txt(file_path)`: –ó–∞–≥—Ä—É–∑–∫–∞ .txt —Ñ–∞–π–ª–∞
- `load_pdf(file_path)`: –ó–∞–≥—Ä—É–∑–∫–∞ .pdf —Ñ–∞–π–ª–∞
- `load_docx(file_path)`: –ó–∞–≥—Ä—É–∑–∫–∞ .docx —Ñ–∞–π–ª–∞
- `load_multiple(file_paths)`: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
- `load_from_directory(directory, recursive=False)`: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:** `.txt`, `.pdf`, `.docx`, `.doc`

### SimilarityCalculator

–ö–ª–∞—Å—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏.

**–ú–µ—Ç–æ–¥—ã:**
- `cosine_similarity_tfidf(documents)`: Cosine similarity —Å TF-IDF
- `longest_common_subsequence(text1, text2)`: LCS
- `sequence_matcher_ratio(text1, text2)`: SequenceMatcher
- `ngram_similarity(tokens1, tokens2, n=2)`: N-gram similarity
- `calculate_all_similarities(text1, text2, tokens1, tokens2)`: –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏
- `compare_multiple_documents(documents, processed_tokens)`: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

### SimilarityVisualizer

–ö–ª–∞—Å—Å –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏.

**–ú–µ—Ç–æ–¥—ã:**
- `plot_similarity_matrix(similarity_matrix, labels, ...)`: –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
- `plot_comparison_results(results, doc1_name, doc2_name, ...)`: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- `plot_multiple_matrices(comparison_results, ...)`: –ù–µ—Å–∫–æ–ª—å–∫–æ –º–∞—Ç—Ä–∏—Ü
- `plot_plagiarism_report(comparison_results, threshold, ...)`: –û—Ç—á–µ—Ç –æ –ø–ª–∞–≥–∏–∞—Ç–µ

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä

```python
from src.main import analyze_plagiarism

# –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
detector = analyze_plagiarism('uploads/', threshold=0.7)
```

### –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from src.main import PlagiarismDetector

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
detector = PlagiarismDetector(language='english')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
detector.load_documents('uploads/')

# –û–±—Ä–∞–±–æ—Ç–∫–∞
detector.process_documents(remove_stopwords=True, lemmatize=True)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
results = detector.compare_documents()

# –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
details = detector.compare_two_documents('doc1.txt', 'doc2.txt')

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
report = detector.generate_report(threshold=0.7)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
detector.visualize_results(save_dir='results/', threshold=0.7)
```

### –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞

```bash
# –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
python -m src.main uploads/

# –ò–ª–∏ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
python scripts/example.py
```

---

## –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏

–°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤:

1. **Cosine Similarity (TF-IDF)**: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF –≤–µ—Å–æ–≤
2. **Sequence Matcher**: –ê–ª–≥–æ—Ä–∏—Ç–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ Python's difflib
3. **Longest Common Subsequence (LCS)**: –î–ª–∏–Ω–∞ –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ–±—â–µ–π –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
4. **N-gram Similarity**: Jaccard similarity –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º
5. **Average Similarity**: –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º

---

## –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ü–æ—Ä–æ–≥–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏

- **0.9 - 1.0**: üî¥ –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –ø–ª–∞–≥–∏–∞—Ç–∞ (–ø–æ—á—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã)
- **0.7 - 0.9**: üü° –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ (–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å)
- **0.5 - 0.7**: üü¢ –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ (–Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
- **< 0.5**: ‚úì –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

- –î–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ—Ä–æ–≥ **0.6-0.7**
- –î–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ—Ä–æ–≥ **0.5**
- –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã –≤—Ä—É—á–Ω—É—é
