"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞ –≤ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö.
"""
import os
from typing import List, Dict, Optional, Union
from pathlib import Path

from .document_loader import DocumentLoader
from .text_processor import TextProcessor
from .similarity_calculator import SimilarityCalculator
from .visualizer import SimilarityVisualizer


class PlagiarismDetector:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞.
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã: –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤,
    –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é.
    """
    
    def __init__(self, language: str = 'english'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø–ª–∞–≥–∏–∞—Ç–∞.
        
        Args:
            language: –Ø–∑—ã–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ ('english' –∏–ª–∏ 'russian')
        """
        self.language = language
        self.loader = DocumentLoader()
        self.processor = TextProcessor(language=language)
        self.calculator = SimilarityCalculator()
        self.visualizer = SimilarityVisualizer()
        
        self.documents = {}
        self.processed_tokens = {}
        self.comparison_results = None
    
    def load_documents(self, source: Union[str, List[str]], 
                       recursive: bool = False) -> Dict[str, str]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞, —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
        
        Args:
            source: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
            recursive: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö (–¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è_—Ñ–∞–π–ª–∞: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ}
        """
        if isinstance(source, str):
            path = Path(source)
            if path.is_dir():
                self.documents = self.loader.load_from_directory(source, recursive)
            elif path.is_file():
                filename = path.name
                self.documents = {filename: self.loader.load(source)}
            else:
                raise ValueError(f"–ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {source}")
        elif isinstance(source, list):
            self.documents = self.loader.load_multiple(source)
        else:
            raise TypeError("source –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫")
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        return self.documents
    
    def process_documents(self, remove_stopwords: bool = True, 
                          lemmatize: bool = True) -> Dict[str, List[str]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            remove_stopwords: –£–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            lemmatize: –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è_—Ñ–∞–π–ª–∞: —Å–ø–∏—Å–æ–∫_—Ç–æ–∫–µ–Ω–æ–≤}
        """
        if not self.documents:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø–æ–º–æ—â—å—é load_documents()")
        
        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        for filename, text in self.documents.items():
            tokens = self.processor.process(
                text, 
                remove_stopwords=remove_stopwords,
                lemmatize=lemmatize
            )
            self.processed_tokens[filename] = tokens
        
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.processed_tokens)}")
        return self.processed_tokens
    
    def compare_documents(self) -> Dict:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É —Å–æ–±–æ–π.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        if not self.documents or not self.processed_tokens:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        
        print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        self.comparison_results = self.calculator.compare_multiple_documents(
            self.documents,
            self.processed_tokens
        )
        
        print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return self.comparison_results
    
    def compare_two_documents(self, doc1_name: str, doc2_name: str) -> Dict[str, float]:
        """
        –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            doc1_name: –ò–º—è –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc2_name: –ò–º—è –≤—Ç–æ—Ä–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        if doc1_name not in self.documents or doc2_name not in self.documents:
            raise ValueError("–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        results = self.calculator.calculate_all_similarities(
            self.documents[doc1_name],
            self.documents[doc2_name],
            self.processed_tokens[doc1_name],
            self.processed_tokens[doc2_name]
        )
        
        return results
    
    def visualize_results(self, save_dir: Optional[str] = None,
                         threshold: float = 0.7) -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        
        Args:
            save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–µ—Å–ª–∏ None, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è)
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        if not self.comparison_results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report_path = os.path.join(save_dir, 'plagiarism_report.png') if save_dir else None
        self.visualizer.plot_plagiarism_report(
            self.comparison_results,
            threshold=threshold,
            save_path=report_path
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        matrices_path = save_dir if save_dir else None
        self.visualizer.plot_multiple_matrices(
            self.comparison_results,
            save_dir=matrices_path
        )
    
    def generate_report(self, threshold: float = 0.7) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –ø–ª–∞–≥–∏–∞—Ç–µ.
        
        Args:
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞—Ö
        """
        if not self.comparison_results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        doc_names = self.comparison_results['document_names']
        avg_matrix = self.comparison_results['average_similarity']
        n_docs = len(doc_names)
        
        suspicious_pairs = []
        for i in range(n_docs):
            for j in range(i+1, n_docs):
                similarity = avg_matrix[i][j]
                if similarity >= threshold:
                    suspicious_pairs.append({
                        'document1': doc_names[i],
                        'document2': doc_names[j],
                        'similarity': similarity,
                        'cosine': self.comparison_results['cosine_similarity'][i][j],
                        'sequence_matcher': self.comparison_results['sequence_matcher'][i][j],
                        'bigram': self.comparison_results['bigram_similarity'][i][j]
                    })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        suspicious_pairs.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            'total_documents': n_docs,
            'suspicious_pairs': suspicious_pairs,
            'threshold': threshold
        }
    
    def print_report(self, threshold: float = 0.7) -> None:
        """
        –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ –ø–ª–∞–≥–∏–∞—Ç–µ –≤ –∫–æ–Ω—Å–æ–ª—å.
        
        Args:
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        report = self.generate_report(threshold)
        
        print("\n" + "="*70)
        print("–û–¢–ß–ï–¢ –û –î–ï–¢–ï–ö–¢–ò–†–û–í–ê–ù–ò–ò –ü–õ–ê–ì–ò–ê–¢–ê")
        print("="*70)
        print(f"\n–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {report['total_documents']}")
        print(f"–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {report['threshold']:.1%}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä: {len(report['suspicious_pairs'])}")
        
        if report['suspicious_pairs']:
            print("\n" + "-"*70)
            print("–ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–ï –ü–ê–†–´:")
            print("-"*70)
            
            for idx, pair in enumerate(report['suspicious_pairs'], 1):
                risk_level = "üî¥ –í–´–°–û–ö–ò–ô" if pair['similarity'] >= 0.9 else "üü° –°–†–ï–î–ù–ò–ô"
                print(f"\n{idx}. {risk_level} –†–ò–°–ö –ü–õ–ê–ì–ò–ê–¢–ê")
                print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ã:")
                print(f"     - {pair['document1']}")
                print(f"     - {pair['document2']}")
                print(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {pair['similarity']:.1%}")
                print(f"   –î–µ—Ç–∞–ª–∏:")
                print(f"     ‚Ä¢ Cosine (TF-IDF): {pair['cosine']:.1%}")
                print(f"     ‚Ä¢ Sequence Matcher: {pair['sequence_matcher']:.1%}")
                print(f"     ‚Ä¢ Bigram: {pair['bigram']:.1%}")
        else:
            print("\n‚úì –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        print("\n" + "="*70 + "\n")


def analyze_plagiarism(source: Union[str, List[str]],
                       language: str = 'english',
                       threshold: float = 0.7,
                       visualize: bool = True,
                       save_dir: Optional[str] = None) -> PlagiarismDetector:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞.
    
    Args:
        source: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        language: –Ø–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ('english' –∏–ª–∏ 'russian')
        threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞
        visualize: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –û–±—ä–µ–∫—Ç PlagiarismDetector —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    detector = PlagiarismDetector(language=language)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    detector.load_documents(source)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    detector.process_documents()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    detector.compare_documents()
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    detector.print_report(threshold=threshold)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if visualize:
        detector.visualize_results(save_dir=save_dir, threshold=threshold)
    
    return detector


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å README
def analyze_data(data_path: str) -> dict:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤ README.
    
    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    detector = analyze_plagiarism(data_path, visualize=False)
    return detector.generate_report()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python -m src.main <–ø—É—Ç—å_–∫_–¥–æ–∫—É–º–µ–Ω—Ç–∞–º>")
        print("–ü—Ä–∏–º–µ—Ä: python -m src.main uploads/")
        sys.exit(1)
    
    source_path = sys.argv[1]
    analyze_plagiarism(source_path, save_dir="results")
