name: Auto Plagiarism Analysis

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'uploads/**'
  pull_request:
    branches: [ main, dev ]
    paths:
      - 'uploads/**'

jobs:
  analyze:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache NLTK data
      uses: actions/cache@v4
      with:
        path: ~/nltk_data
        key: ${{ runner.os }}-nltk
        restore-keys: |
          ${{ runner.os }}-nltk-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download NLTK data
      run: |
        python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('omw-1.4')"
    
    - name: Check for files in uploads/
      id: check_files
      run: |
        if [ -d "uploads" ] && [ "$(ls -A uploads/ 2>/dev/null | grep -E '\.(txt|pdf|docx|doc)$')" ]; then
          echo "has_files=true" >> $GITHUB_OUTPUT
          echo "‚úì –ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
          ls -la uploads/
        else
          echo "has_files=false" >> $GITHUB_OUTPUT
          echo "‚ö† –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        fi
    
    - name: Run plagiarism analysis
      if: steps.check_files.outputs.has_files == 'true'
      run: |
        timestamp=$(date +%Y%m%d_%H%M%S)
        echo "–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞..."
        python -c "
        import sys
        import json
        from datetime import datetime
        from pathlib import Path
        sys.path.insert(0, '.')
        from src.main import PlagiarismDetector
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        detector = PlagiarismDetector(language='russian')
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        try:
            detector.load_documents('uploads/')
            if not detector.documents:
                print('‚ö† –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
                sys.exit(0)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            detector.process_documents()
            detector.compare_documents()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            report = detector.generate_report(threshold=0.7)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = {
                'timestamp': datetime.now().isoformat(),
                'total_documents': report['total_documents'],
                'threshold': report['threshold'],
                'suspicious_pairs_count': len(report['suspicious_pairs']),
                'suspicious_pairs': [
                    {
                        'document1': pair['document1'],
                        'document2': pair['document2'],
                        'similarity': pair['similarity'],
                        'cosine': pair['cosine'],
                        'sequence_matcher': pair['sequence_matcher'],
                        'bigram': pair['bigram']
                    }
                    for pair in report['suspicious_pairs']
                ],
                'document_names': detector.comparison_results['document_names']
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
            Path('results').mkdir(exist_ok=True)
            output_file = f'results/analysis_${timestamp}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f'‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}')
            print(f'–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {report[\"total_documents\"]}')
            print(f'–ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä: {len(report[\"suspicious_pairs\"])}')
            
        except Exception as e:
            print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        "
    
    - name: Display analysis summary
      if: steps.check_files.outputs.has_files == 'true'
      run: |
        echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
        echo "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –ü–õ–ê–ì–ò–ê–¢–ê"
        echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
        latest_result=$(ls -t results/analysis_*.json 2>/dev/null | head -n1)
        if [ -f "$latest_result" ]; then
          python -c "
        import json
        import sys
        
        with open('$latest_result', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f'–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {data[\"timestamp\"]}')
        print(f'–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {data[\"total_documents\"]}')
        print(f'–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {data[\"threshold\"]:.0%}')
        print(f'–ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä: {data[\"suspicious_pairs_count\"]}')
        print()
        
        if data['suspicious_pairs']:
            print('–ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–ï –ü–ê–†–´:')
            print('‚îÄ' * 56)
            for idx, pair in enumerate(data['suspicious_pairs'], 1):
                risk = 'üî¥ –í–´–°–û–ö–ò–ô' if pair['similarity'] >= 0.9 else 'üü° –°–†–ï–î–ù–ò–ô'
                print(f'{idx}. {risk}')
                print(f'   {pair[\"document1\"]} ‚Üî {pair[\"document2\"]}')
                print(f'   –°—Ö–æ–∂–µ—Å—Ç—å: {pair[\"similarity\"]:.1%}')
                print()
        else:
            print('‚úì –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω–æ')
        print('‚ïê' * 56)
          "
        else
          echo "‚ö† –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        fi
    
    - name: Upload analysis results
      if: steps.check_files.outputs.has_files == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: plagiarism-analysis-results
        path: results/analysis_*.json
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && steps.check_files.outputs.has_files == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
          const resultsDir = 'results';
          const files = fs.readdirSync(resultsDir)
            .filter(f => f.startsWith('analysis_') && f.endsWith('.json'))
            .map(f => ({
              name: f,
              path: path.join(resultsDir, f),
              time: fs.statSync(path.join(resultsDir, f)).mtime.getTime()
            }))
            .sort((a, b) => b.time - a.time);
          
          if (files.length === 0) {
            console.log('–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è');
            return;
          }
          
          const latestFile = files[0].path;
          const data = JSON.parse(fs.readFileSync(latestFile, 'utf8'));
          
          // –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
          let comment = '## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞\n\n';
          comment += `**–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:** ${data.timestamp}\n`;
          comment += `**–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** ${data.total_documents}\n`;
          comment += `**–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏:** ${(data.threshold * 100).toFixed(0)}%\n`;
          comment += `**–ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä:** ${data.suspicious_pairs_count}\n\n`;
          
          if (data.suspicious_pairs_count > 0) {
            comment += '### ‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã:\n\n';
            comment += '| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç—ã | –°—Ö–æ–∂–µ—Å—Ç—å | –†–∏—Å–∫ |\n';
            comment += '|---|-----------|----------|------|\n';
            
            data.suspicious_pairs.forEach((pair, idx) => {
              const risk = pair.similarity >= 0.9 ? 'üî¥ –í—ã—Å–æ–∫–∏–π' : 'üü° –°—Ä–µ–¥–Ω–∏–π';
              const sim = (pair.similarity * 100).toFixed(1);
              comment += `| ${idx + 1} | ${pair.document1}<br>‚Üî<br>${pair.document2} | ${sim}% | ${risk} |\n`;
            });
          } else {
            comment += '### ‚úÖ –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n';
          }
          
          comment += '\n---\n';
          comment += '*–ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö workflow*';
          
          // –ü—É–±–ª–∏–∫–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
